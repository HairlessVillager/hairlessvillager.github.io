[{"title":"Eko 工作复盘","date":"2025-09-11T09:44:00.000Z","url":"/2025/09/11/eko-work-review/","categories":[[" ",""]],"content":"本文参考的所有资料均公开可见，主要参考源包括： Eko 开源代码： 我提的 PRs： Eko 文档： Eko 三个不同版本的架构分析我在 Eko 里经历过两次大的 Eko 框架升级： v0-&gt;v1 的性能提升很显著，没有重构，全部都是工程上的 trick。 v1-&gt;v2 重构了全部代码，重写了全部文档。 v0算法 AI 生成一个工作流，工作流基本为线性的，即一个节点完成后下一个节点才能开始，每个节点包含三个字段：任务的标题 name, 任务的描述 description, 任务可以使用的工具 tools，此外工作流实例本身有一个变量 memory 在每个节点间共享。 对于每个节点，遵循 ReAct 模式（最多 10 轮），Eko 将上述四个字段嵌入 system message 和第一条 user message，让 AI 调用工具来执行任务，当 AI 认为当前任务已经执行完毕时，应该调用特殊工具（return_output，保证每个节点必包含这个工具）来结束任务，工具的唯一参数应该为这个节点的输出。 提示词 特殊工具return_output定义如下： 生成工作流的 prompt： 工作流每个节点的 system message &amp; user message： 缺陷 第一步生成的工作流像一个玩具，节点数量大部分时间都是3个，每个节点的工具数量从不超过5个，每个节点的 description 大而空，大模型不能在每个节点内完成指定的任务，只能提供泛泛而谈的输出，进而导致整个工作流只能在简单任务上表现较好，但是基本不能完成较复杂任务。 在第1点的基础上，工作流的无效动作多，耗时长。 v1算法 抛弃 Workflow，完全使用 ReAct 模式，具体到实现上是弃用 AI 生成工作流的逻辑，改为手动构造工作流，工作流中仅包含一个节点，节点的 title 和 description 就是原始的用户输入的 prompt，节点的 tools 是全部工具，同时将节点轮数扩大到 50 轮 commit。 给所有工具的 input schema 打补丁，强制大模型在生成工具参数 toolCall 的同时生成以下字段： Part 1 commit thinking: 模拟大模型思考过程，在生成其他字段和工具 input 前生成。 observation: 让大模型输出他看到了什么，主要是调试用，因为当时不能确保工具一定调用成功，而且我们不能直接看到工具的执行情况。 userSidePrompt: 产品要求大模型在调用工具的时候要生成一段文本展示给用户，格式遵循“I’m doing X.”。 Part 2 commit evaluate_previous_goal: 评估上一个目标或操作的成功情况，供大模型反思。 memory: 记录已完成的操作和需要记住的信息。 next_goal: 描述下一个即时操作需要完成的任务。 上下文压缩器 ContextComporessor：每次原始 messages 会经过 comporessor 压缩一次后再发给 API，这里有两个实现： SimpleQAComporess：基于上面的 input schema 补丁，会直接丢弃原始的 tool result message，转而使用下一轮模型的 observation 字段替代，极大减少了 token 消耗，几乎避免了上下文超限的问题 commit。 SummaryCompress：每当对话轮数超过 10 轮，调用大模型根据历史对话生成压缩后的历史，作为一条 user message，新的 message 由原始 system message，第一条 user message，提示历史开始的 user message，压缩后的历史 user message 及最近两条 message 构成 commit。 提示词生成 plan 的 system message &amp; user message 同 v1，input schema 补丁字段和 SummaryCompress 的 prompt 见对应的 commit。 下面是节点的 system prompt &amp; user message： 效果 虽然没有统计，但是主观感觉到确实比 v1 更好用了，AI 会自己尝试使用各种工具解决问题，能够解决一部分较复杂任务。 上下文超限导致的 API 报错基本不会复现了。 缺陷 在任务过于复杂时，上下文的压缩效果并不好，SimpleQAComporess 不包含工具的原始输出，大模型的 observation 可能也不能提供后面要用的关键信息；SummaryCompress 也有类似的问题。 input schema 补丁不能无限制使用，实践中只加 Part 1 的 3 个字段是最好用的，再加字段不仅不会让模型在新的字段上输出想要的内容，在旧字段及 toolCall 字段上的表现也会更差。 v2v2 版的架构做了更大的改动，我仅参与了部分文档的重写，没有参与后续的维护，推荐参考：。 算法 先由 AI 生成一段基于 XML 的工作流，包含各种节点，除了普通的任务节点（还是 ReAct）外，还有： 循环节点：设置循环条件（一段 prompt），AI 会反复执行循环体并判断是否结束循环； Agent 节点：该节点下的所有节点都在这个节点描述的 Agent 实例中运行； 监听节点：监听是浏览器中的概念，监听节点会监听 DOM 树中的指定节点，发现更新后立即执行该节点的内容；执行的逻辑和 v0 基本一致。 提示词 生成 Plan 的 prompt： 各节点的 prompt： 其他比较有价值的 PR #36：增加了 Human Tools，通过 callbacks 实现在工作流执行中和用户交互的方法。在 v0 和 v1 中都是 4 个独立的工具，在 v2 中被整合成一个工具。 #37 &amp; #88：实现了工作流结束时的总结功能，一开始的实现是添加一个额外的工具，并在 prompt 里提示大模型调用；后来撤回了这个修改，改成了工作流结束后将完整执行过程发给单独的 Agent 让它总结。 #132：用云日志托管平台来实现可在线查看的日志，使用 uuidv4 跟踪每个 Workflow 实例的执行过程。 #151：通过一系列方法限制了 token 消耗，包括上下文压缩、限制工具输出长度、限制对话轮数、移除 input schema 补丁的 p2。 一些见解 在 ReAct 范式下，上下文是线性增长的。在这个背景下，解决越来越复杂的任务，首先要解决越来越长的上下文。这里的上下文包括： system prompt &amp; tools：每个 Agent 都有自己负责的一块 domain，并有与之配对的 system prompt 和 tools。 user prompt：Workflow 的输入，分为用户输入的 prompt 和大模型生成的 prompt。 tool call &amp; results：每次调用工具时的输入和输出。 有很多方法可以解决上下文越来越长的问题： 上下文压缩：对 tool call &amp; results 做压缩，但是 1) 不能保证不删除关键信息；2) 不能解决 system prompt &amp; tools 随着问题空间变大、答案验证更细致而越来越长的问题。 Multi-Agent：按 domain 拆分 system prompt &amp; tools，进而自动拆分了 user prompt 和 tool call &amp; results。 "},{"title":"记录 Region Diff 开发中的一些思考","date":"2025-07-09T00:36:00.000Z","url":"/2025/07/09/region-diff/","categories":[[" ",""]],"content":"背景今年 4 月份的时候突然很想玩 Minecraft，跟群友商量过后决定开一个游戏服务器，服务器搭建在群友的 PC 上。 为了方便群友白嫖，服务器没有开启正版验证，也就是说任何人都可以进入存档并造成不可逆的破坏。为了防止群友的心血毁于一旦，我提议对存档进行备份，但是此时存档的大小已经有 2-3GB 左右了，PC 的硬盘空间也只有几百 GB，即使每小时备份一次、删除 48 小时前的备份，造成的空间占用也不能接受。 经过简单的分析，发现存档体积占用的 99% 都集中在 *.mca 文件，而后者的解析有非常多的社区文档。此外一个非常明显的结论是玩家正常游玩对游戏存档产生的影响是非常局限的。 后期测试发现，一个玩家在单个 .mca 文件中连续玩 6 天（每天 5 小时），所生成的差分文件大小仅为 600KB，而对应的 .mca 文件大小为 8000KB。 在这个背景下，我决定开发一个简单的命令行工具，对占用空间较大的 *.mca 文件提供差分支持。 项目已经开源到 GitHub。 差分给定两个序列 S1 和 S2，他们的差分 D 满足以下性质： 把 D 应用到 S1 上可以得到 S2 把 D 应用到 S2 上可以得到 S1 这里有一个平凡的实现：差分存储 A 和 B 的完整信息，输入 A 时输出 B，输入 B 时输出 A。 当然也有一些更高效的实现，比如 LCS、Myer’s、Patience 等。这里使用比较主流的 Myer’s Diff 算法。 为了实现类似 git rebase -i 中的 squash 操作，我自己设计了一个简单的 squash 算法。 定义差分的 squash 操作：给定两个序列 S1, S2, S3，计算 S1 到 S2 的差分 D12、S2 到 S3 的差分 D23，令 D13 :&#x3D; squash(D12, D23)，此时 D13 满足以下性质： 把 D13 应用到 S1 上得到 S3 把 D13 应用到 S3 上得到 S1 经过调研没有发现合适的 squash 算法实现，于是花了几天时间写了一个，代码点这里。 利用文件结构加速差分计算我们现在需要对文件做差分。一个非常 naive 的想法是参考 git diff 命令，使用 Myers Diff 算法对每个文件的二进制内容生成差分。 Myers Diff 算法的时间复杂度是 O(ND)，在最坏情况下，两个序列完全不同，此时 N=D，时间复杂度退化为 O(N^2)。对于 Git 的应用场景，这点时间开销是完全可以接受的，但是对于以 MB 计量的数据，时间开销非常客观，估算可以达到若干天。 然而每个区块的数据都是可解析的结构化数据，可以理解成 JSON，我们可以采用分而治之的思想对 JSON 里的一些瓶颈字段做单独的差分操作，这样问题规模就降低了很多。 这里说的很简单，但是在工程上有很多细节，比如利用时间戳跳过差分、利用 xyz 坐标提供的结构化信息等。 多线程的负载均衡每个 .mca 文件都包含 1024 个区块数据，这些数据都是独立且只读的，因此可以使用多线程实现并行计算。然而区块的计算量服从幂律分布，也就是有少数区块占用了大部分计算时间。如果不能合理地把计算负载分配给 worker，那么在一段时间后，一些线程的总工作量较少提前返回了，而少数几个线程还在运行，导致 CPU 资源不能高效利用，所谓“一核有难，七核围观”。 第一版的实现使用 rayon 库的 .par_iter()，它会把工作负载序列均分为 n 等分，交给 n 个线程处理，某个线程处理完后会窃取其他线程的工作。当时我还没意识到有工作窃取的机制，还测试了对工作负载序列洗牌能不能提升性能，效果微乎其微。 假设我们有两个线程，任务的开销分别是 1, 1, 5, 1, 1, 1，那么线程在不同时刻处理的任务的甘特图如下： 为了让这些线程的工作量尽量相同，第二版的实现我们让不同的线程去争抢任务。因为要同步处理的进度，这个方法会比上一个方法产生更多的锁开销。甘特图如下： 可以看到确实有一些改善，但是线程 2 还是有两个 tick 的空闲时间。如果线程 1 能在一开始就处理开销为 5 的任务，那么正好能让两个线程同步返回： 这也是第三版的实现——预先估算任务的开销，并降序排序任务序列，这样能最大地平衡各个线程的工作量。"},{"title":"GPU-bound Web 程序性能优化","date":"2024-10-11T02:30:00.000Z","url":"/2024/10/11/research-on-gpu-bound-web-server/","categories":[[" ",""]],"content":"最近接手了一个 GPU-bound 的 Web Server 开发工作，这里记录一下开发中的问题、实验数据和解决方案。 Machine：Google Deep Learning VM GPU: NVIDIA A100-SXM4-40GB * 4 Memory: 334 GB Driver Version: 560.35.03 CUDA Version: 12.6 Requirements使用 Huggingface 托管的 TrustSafeAI&#x2F;RADAR-Vicuna-7B 模型做文本分类操作，输入长度不固定，需要拆分成不大于 300 字符的字符串列表分别推理，然后一起返回给客户端。 No Batch284699c main.py 原型开发时不考虑效率，用了非常 naive 的实现。后续使用 locust 做性能测试，得到这些结果： 1 worker with nginx 1 worker without nginx 4 workers with nginx 4 workers without nginx 每一个 Worker 都独占一个进程，并拥有独立的显存。 从性能测试的结果可以看出： 单进程时用户数在 20 左右可以发挥最好的性能，RPS 不会超过 25，RTp95 大约为 1000ms，随着用户数增多 RT 越来越大，同时 RPS 越来越小，Nginx 基本没有作用； 多进程时用户数在 20 左右可以发挥最好性能，相较于没有 Nginx 的版本，有 Nginx 时性能和稳定性反倒是下降了，主要体现在用户数大于 40 的压力测试阶段： 无 Nginx 时 RPS 稳定在 34 左右，有 Nginx 时 RPS 稳定在 25 左右 无 Nginx 时 RTp50 &lt; 1000ms，RTp95 ~ 4000ms，有 Nginx 时 RTp50 &lt; 3000ms，RTp95 ~ 4500ms 结论：Nginx 基本没有作用，有时甚至会起到反作用；多进程的主要作用是提高 RPS，对 RT 的改善不明显。 Offline Batch28693e1 script.py 在测试过程应用的日志中看到这样一条提示：“You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset”。搜索可知这条提示会在 pipe() 被调用 10 次后固定输出（ref）。从 issue 中可以得到一条启示，即最好使用批处理（batch）来加速计算过程。 写一个脚本测试一下使用批处理时的性能，大概思路是不断生成随机长度的字符串，用不同的 batch_size 并行推理，然后将开销平摊到每个字符串上，具体实现见仓库中的 script.py 文件。 测试得到的结果如下： 当字符串长度在 [100, 800] 均匀分布时： 当字符串长度在 [100, 300] 均匀分布时： 可知 batch_size 选择 16 或 32 时性能最高，这个可作为后续参数的参考值。 Online Batch548809f main.py 参考了 Transformers 官方文档上关于 Web Server 的建议（ref），使用异步和批处理又写了一版。 这里把关键部分放上来，一些无关紧要的地方已经删去或使用 ... 代替。 这里涉及到三个函数： startup：钩子函数，在应用初始化后、开始接受请求前执行，用来创建server_loop异步任务。 server_loop：不断从mq中接收消息，批处理式推理，然后返回给调用者。 analyze_and_classify：处理 HTTP 请求时会多次调用这个函数，推理接口的封装。 按照时间顺序来描述接下来会发生的事： startup：创建一个“全局变量”：mq = asyncio.Queue()； server_loop：调用pipeline函数，加载 pipeline； server_loop：进入死循环，在时间 BATCH_TIMEOUT 内尝试从mq获取BATCH_SIZE_MAX个对象； analyze_and_classify：被调用（调用者是ai_detection_on_single_string），创建一个rq = asyncio.Queue()，然后把text和其他消息放入app.mq中，然后异步等待从rq获取对象； server_loop：超时或已经获取到了BATCH_SIZE_MAX个对象，如果对象列表非空，则对这些对象批处理推理，batch_size为对象的数量； server_loop：推理完成，把结果放入对应的rq，进入下一轮循环； analyze_and_classify：从rq中获取到对象并返回。 在全局范围内，mq被创建一次，rq在每次调用analyze_and_classify时都会创建一次。创建asyncio.Queue()的时间开销很小，用timeit在我的机器上测得 2.5us。 使用不同的 BATCH_SIZE_MAX 和 BATCH_TIMEOUT 进行性能测试： BATCH_SIZE_MAX=8, BATCH_TIMEOUT=0.5: BATCH_SIZE_MAX=8, BATCH_TIMEOUT=0.3: BATCH_SIZE_MAX=8, BATCH_TIMEOUT=0.1: BATCH_SIZE_MAX=16, BATCH_TIMEOUT=0.5: BATCH_SIZE_MAX=16, BATCH_TIMEOUT=0.3: BATCH_SIZE_MAX=16, BATCH_TIMEOUT=0.1: BATCH_SIZE_MAX=32, BATCH_TIMEOUT=0.5: BATCH_SIZE_MAX=32, BATCH_TIMEOUT=0.3: BATCH_SIZE_MAX=32, BATCH_TIMEOUT=0.1: 观察可以得到以下几个结论： 随着用户数的线性增长，RPS 增加会逐渐变慢； BATCH_SIZE_MAX不变时，减小BATCH_TIMEOUT能小幅度提升 RPS，小幅度降低 RT，但是能提升 RT 的稳定性，当BATCH_TIMEOUT为 100ms 时尤甚； BATCH_TIMEOUT不变时，增加BATCH_SIZE_MAX能提升 RPS，但是会影响 RT 的稳定性。 尽管我们得到了这几个结论，但这也只是局限在这个实验范围内，不同的语言、实现细节、硬件规格都可能产生不同的结论。正如 Transformers 官方文档所言： Creating an inference engine is a complex topic, and the “best” solution will most likely depend on your problem space. 最后放上 BATCH_SIZE_MAX=16 BATCH_TIMEOUT=0.1 参数的压测结果作为结尾： "},{"title":"使用 Docker 搭建 miniob 开发环境","date":"2024-09-24T12:33:00.000Z","url":"/2024/09/24/build-miniob-development-environment/","categories":[[" ",""]],"content":"背景学校的一门数据库课程要求在 miniob 上实现一些功能，评测方式和算法竞赛差不多，提交仓库地址和 commit hash 后评测系统会输入若干组数据，把程序输出和答案输出比较，一致则得分。 miniob 的开发环境设置比较繁琐，这里记录一下我自己的开发环境，以备不时之需。 准备工作配置 Gitee 仓库国外的 GitHub 由于众所周知的原因时常无妨访问，这里使用 Gitee 作为代码托管。在 Gitee 创建仓库时选择从 GitHub 克隆，地址：。 Gitee 仓库创建好后，使用git clone命令把仓库克隆到本地，保证 push&#x2F;pop&#x2F;fetch 操作无报错。 配置 VSCode安装 C&#x2F;C++ Extension Pack 插件，主要为了实现代码跳转功能。这里亦可使用自己喜欢的 IDE。 配置 DockerDocker 安装好后，使用docker run -d --name miniob-dev --privileged -e REPO_ADDR=&lt;your_repo_addr&gt; oceanbase/miniob命令创建 miniob 的开发容器。这里的&lt;your_repo_addr&gt;要换成自己的仓库地址，形如。容器创建时会克隆仓库到容器内。 编译容器创建后，使用docker exec -it miniob-dev bash进入容器的 Bash 环境。 查看一下当前的位置。输入pwd显示当前目录是/root。输入ls查看目录下的文件&#x2F;子目录，应该有docker和source两个文件夹。 输入cd source/miniob进入 miniob 的仓库目录。 输入bash build.sh运行编译脚本。编译过程需要大约两分钟，无报错则说明编译成功。 输入./build_debug/bin/observer -f ./etc/observer.ini -P cli启动 miniob 的 CLI。这里可以自由输入 SQL 测试。 开发流程这里最好创建两个 Terminal，一个在宿主机，一个在容器内。 在宿主机上修改源代码略。 源代码复制到容器内使用docker cp ./src miniob-dev:/root/source/miniob/命令复制。复制大小大概为1.8MB。 快速构建使用sudo apt-get install dos2unix安装格式转换工具。 把以下内容写入easy-build.sh： 然后使用bash easy-build.sh快速构建。 本地测试test/case目录下有测试用例，可自行测试。 提交评测在训练营页面点击“立即评测”，输入仓库地址、commit hash后提交，评测会需要很长时间，可在此页面查看评测结果。"},{"title":"在 Zsh 环境下配置 Conda/Mamba 踩坑记录","date":"2024-09-10T01:33:00.000Z","url":"/2024/09/10/a-problem-in-windows-conda-zsh/","categories":[[" ",""]],"content":"相关 Issue：    其一在 zsh 中运行命令 mamba activate base 提示运行 mamba init zsh，运行后重启终端发现无效。 观察输出，发现.zshrc配置文件的位置不对，输出在了 Windows 的 home 目录下（即C:\\user\\username\\.zshrc），而我希望输出在 Zsh 的 home 目录下（即D:\\cygwin\\home\\username\\.zshrc）。 这个问题解决很简单，把前者新增内容添加到后者即可。 其二再次运行mamba init zsh，提示 (eval):10: parse error near `^M&#39;。 看到^M很容易就能猜出是换行符相关的问题。 参考上面 3 个 issue 后，给出我自己的解决方法： 在 Zsh 的 home 目录下创建 conda-shell-zsh-hook.zsh文件，在保存时注意使用 LF 作为换行符： 在.zshrc目录中增加以下内容（如已存在则替换）： 启动 Zsh 后无报错，且在右侧显示环境名称，问题解决。"},{"title":"Python Web 应用并发测试","date":"2024-08-11T08:59:00.000Z","url":"/2024/08/11/benchtest-on-python-web-application/","categories":[[" ",""]],"content":"网上常听到这样一种论调：Python 太慢，因而不适合做 Web 开发。为了详细了解 Python 在实用场景下的性能，笔者做了一个简单的博客系统，支持简单的博客展示，提供上传、修改的接口，然后尝试在迭代过程中提高并发性能。 项目已开源在 GitHub。这个系统最终的实现是 MongoDB + Redis + local cache 作为分级存储，尽可能使用异步逻辑，使用 Uvicorn 的多进程提高效率。为了直观地看到性能的变化，本文按照倒序的方式来编排。 测试的具体实现就是用 locust 模仿若干用户并发访问一个指定的博客，见项目下的 bench.py 文件。 测试硬件条件： CPU：2 核 内存：2GB 带宽：100Mbps 性能极限 用最终实现测得极限性能，前期最大 RPS 在 2500 左右，然后系统变得非常不稳定，此时有 2000 个用户。因此以下测试均使用 1000 个用户作为测试条件。 最终实现 ef25cb1 RPS ~ 2300 RTp50 ~ 200ms RTp95 ~ 1300ms 后面一段异常的数据可能是压测主机不太稳定。 移除本地缓存 84f2587 RPS ~ 1300 RTp50 ~ 300ms RTP95 ~ 3300ms 本地缓存（local cache）其实就是 Python 里一个字典类型的全局变量。移除后的最高层级缓存是 Redis 实例，在读取数据时必须经过网络通信，所以这里的性能下降了很多。 中间一段异常数据原因暂未知，忘记拿日志了😭 移除 Redis 缓存 b2b6168 RPS ~ 1000 RTp50 ~ 300ms RTp95 ~ 3300ms 移除 Redis 缓存后，所有读请求必须经过 MongoDB 数据库，但是这里的 RPS 并没有降低多少。猜测原因是无论从 Redis 读还是从 MongoDB 读，大部分的时间都花在了网络通信上。 把 MongoDB 换成 Postgres 9ae9101 RPS ~ 510 RTp50 ~ 2000ms RTp95 ~ 5000ms MongoDB 属于 NoSQL 数据库。相比于传统的关系型数据库，NoSQL 不支持事务，没有 Schema，只保证最终一致性，好处是带来了一定的性能提升。可以看到把数据库换成关系型数据库后，性能显著下降。 当然 Postgres 也有 NoSQL 特性，这里没有启用。 多进程变单进程 29fc343 RPS ~ 220 RTp50 ~ 4000ms RTp95 ~ 12000ms 服务器只有两个核心，这里我只开了两个进程。变成单进程后性能确实减半了。 移除异步 fae58bb RPS ~ 180 RTp50 ~ 6000ms RTp95 ~ 6000ms 这个结果比较奇怪，为什么移除异步后 RPS 变化还没有 RTp95 变化大。 监控最后附上服务器的监控。 "},{"title":"Scrapy 调度算法研究","date":"2024-06-14T07:15:00.000Z","url":"/2024/06/14/research-on-scrapy-scheduling-algorithm/","categories":[[" ",""]],"content":"背景Scrapy 是基于 Python 的自动爬虫框架，用户通过编写 Spider 类来定义爬取的行为，即提取内容并产生若干个 Item 或者新的 Reuqest。根据 Architecture overview，Spider 实例会不断产生 Request 对象发送给 Engine，Engine 再转发给 Scheduler 进行调度，之后向 Scheduler 请求一个 Request，此时Scheduler 会根据某种调度算法返回一个 Request，Engine 再转发给 Downloader 下载。 Scheduler 在初始化的时候有 3 个队列类的参数，分别是 dqclass、mqclass、pqclass，类型提示都是 class。文档提供的默认值也没有用。在 GitHub 上找到了源代码才发现前面两个都是在 squeues.py 里动态定义的，而 pqclass 定义在 pqueues.py。 squeues.py观察前面的几个 import 语句： 大致猜到这个文件作用大概是定义了一些常用的队列。marshal 和 pickle 都是和序列化相关的库。queuelib 定义了一些常用的队列。 稍微解读一下其中的函数： _with_mkdir：类装饰器，给 disk-based 队列用的，作用是确认文件夹的存在，若不存在则创建文件夹。 _serializable_queue：类装饰器，在入队、出队的时候进行序列化、反序列化操作。 _scrapy_serialization_queue：_serializable_queue 的 Scrapy 定制版，主要是把类型注解换成了 Request，然后在序列化的时候使用了 Request.to_dict 和 request_from_dict 方法。 _scrapy_non_serialization_queue：_scrapy_serialization_queue 的无序列化操作版本。 _pickle_serialize：对 pickle.dumps 可能抛出的异常简单封装了一下。 以上函数最终产出了以下几个类： 可以看到都是非常基本的 FIFO 或者 LIFO 队列。 pqueues.py这个文件定义了基于优先级的优先队列。主要定义了 ScrapyPriorityQueue 和 DownloaderAwarePriorityQueue 两个类。 ScrapyPriorityQueueScrapyPriorityQueue 提供了 push 和 pop 两个方法，就像传统的队列一样，但是因为要考虑到优先级，所以具体实现上又与传统的队列有区别。 ScrapyPriorityQueue 实例拥有一个用于指示当前优先级的 curprio 属性，以及与不同优先级对应的若干个 queue。push 方法除了将 request 入队相应的队列之外，还会调整 curprio 为 request.priority。pop 方法会根据 curprio 从相应的队列出队 request 作为返回值，但是在返回之前还会判断一下队列是否为空，如果是则重新调整 curprio 为可用的最小值。 DownloaderAwarePriorityQueueDownloaderAwarePriorityQueue 没太看懂，主要是因为 Downloader 没有文档。不过从 DownloaderAwarePriorityQueue 的文档中可以知道，它的机制是把持有下载最少的域名的 request 最先出队。"},{"title":"NeoVim 中 Conda 环境下配置 LSP 踩坑记录","date":"2024-05-06T15:05:00.000Z","url":"/2024/05/06/a-problem-in-windows-conda-neovim/","categories":[[" ",""]],"content":"问题引入笔者在 Windows 环境下使用 Anaconda 包管理软件管理 Python 的第三方库，但是一直以来 NeoVim 的 LSP 不支持 Conda 除 base 环境的其他环境的第三方包（即 envs&#x2F; 下的环境），一直以来都很困扰我。今天实在忍无可忍了，决定把这个问题探个究竟。 我使用的是 python-lsp-server 这个 LSP。nvim-lspconfig.lua 部分配置展示： 在脚本里打日志省略中间的各种弯路，最终在 python-lsp-server 的 GitHub 上找到一个相关的 discussions #177。@skeledrew 提出他写了一个脚本，能替换 pylsp/workspace.py 中的函数，使其找到祖先路径下的 .env 文件并加载。 这个脚本质量不算很高，很多写法不规范，但是最大的问题是他根本就不能工作，而且我打开 NeoVim 后 CPU 占用飙升。好在作者给了一个 write 函数，可以勉强当日志用。确定到问题出在后面的 while 循环是死循环，cur_path变量一直是WindowsPath(&#39;e:/&#39;)。当然很容易通过计数器变量来限制循环次数，这个问题先按下不表。 观察日志输出： 我的工作目录在 E:/code/python/chatcollector/chatcollector/，这里显然是有问题的，多走了两次父目录。经过我不断在 pylsp 包中打日志，最后确认为 pylsp.python_lsp.PythonLSPServer.m_initialize 的入参为 E:/code/python/chatcollector/，是正常工作目录的一级父目录。这说明调用 pylsp 的调用者在传参的时候出现了问题。 从 pylsp 到 nvim-lspconfig找这个调用者的过程也是极其艰辛，以 m_initialize 为关键词是无法在一般的搜索引擎上找到结果的。最后还是在 pylsp.workspace.Workspace 找到了 M_INITIALIZE_PROGRESS 这个类属性，对应的值为 &quot;window/workDoneProgress/create&quot;。我注意到这里的字符串格式应该是和 LSP 相关，于是在 NeoVim 中使用命令 :LspLog 打开日志，并且把日志等级设为 DEBUG。最后找到了多条形如以下的日志： 然后我联想到我的 pylsp 是配置在 neovim&#x2F;nvim-lspconfig 上的，于是打开 neovim&#x2F;nvim-lspconfig 的默认 pylsp 配置，找到了以下代码： 它的意思是，从当前的目录向祖先方向找，如果这个目录包含 pyproject.toml 之类的文件就判断它为工作目录。 修改如下： 再到脚本里打日志用上文提到的脚本替换 pylsp/workspace.py 文件后发现，实际使用时还是无法提示，打开日志 pylsp_env_path_patcher.log 显示： 发现原来是通过子进程系统调用 conda info -e 时报错，我 Conda 没加到系统路径里。用绝对路径替代之。结果又是死循环，加计数器和日志调试，发现中间有一次循环中 env_fp=WindowsPath(&#39;e:/code/python/chatcollector/chatcollector/.env&#39;)，这里确实是我 .env 文件的位置，但是它忽略了，原因是 “No python interpreter at D:\\Anaconda\\anaconda3\\envs\\django”。关键代码如下： 这里应该是 Linux 系统下的写法，移植到 Windows 下： LspLog 到 pylsp 再到 jedi 的一个 PR依旧没有语法提示。查看 :LspLog（这里已经将\\r\\n转义并且用正确的编码打开）： 我的 python.exe 路径是 D:\\\\Anaconda\\\\anaconda3\\\\envs\\\\django\\\\python.exe，观察代码发现还是 pylsp 的问题。定位到以下代码： 打个日志，输出为 “environment_path&#x3D;’D:\\Anaconda\\anaconda3\\envs\\django’”，结果没有问题。看来需要怀疑 jedi 了。定位到以下代码： 看了一下，好像没有别人提出这个问题，于是顺便拉了个 PR。 再试了一下，好像没问题了。 收尾工作首先重新安装以下被刚刚的日志破坏得千疮百孔的包： 然后需要打两个补丁，首先运行脚本 pyls_env_path_patcher.py，给 pylsp\\workspace.py 打补丁，然后查看我提的 PR，按照 diff 手动修改代码打补丁（如果你的 jedi 版本高于 0.19.1 可能不用修改），最后在 NeoVim 输入命令 :LspRestart 重启 LSP，就能修复这个问题了。"},{"title":"记一次失败的 Django Contribution","date":"2023-12-23T04:03:00.000Z","url":"/2023/12/23/a-failed-django-contribution/","categories":[[" ",""]],"content":"问题场景正在学习 Django 的测试模块，看到 fixture 可以用 manage.py dumpdata 指令生成，遂尝试使用，发现遇到编码问题，输出的中文为 gkb 编码。在官方文档上搜索 database encoding 关键字，发现我使用的 SQLite 数据库后端的默认编码为 UTF-8，其他的地方也没发现 gbk 编码。 分析过程开始调试源码。通过 manage.py 获取命令行解析模块 django.core.management，进一步查看源码定位目标子模块 django.core.management.commands.dumpdata，在 line 265 发现一处对 serializers.serialize 的调用，猜测这里是程序的出口，定位到 django.core.management.serializers。 在 django.core.management.serializers 发现若干子模块，为不同格式的序列化模块。使用多个 dumpdata 的 --format 参数，发现问题均能复现，且均为 gbk 编码中文。进入 json.py 子模块，通过print(__class__.__name__)打断点定位到入口函数 start_serialization()，发现对 self.stream.write() 的调用。尝试增加一行 self.stream.write(&quot;ABCDEFG甲乙丙丁GFEDCBA&quot;)，在输出文件中成功发现这个字符串，且为 gbk 编码。遂开始分析 self.stream。 dumpdata.py line 271 发现传入 stream 参数，分析为输出流优先为文件输出流 stream = open_method(file_path, &quot;wt&quot;, **kwargs)，若无文件输出流则换为标准输出 self.stdout。我的场景是文件输出。定位 open_method() 函数，利用断点辅助调试，定位到 line 253，发现为 Python 内置函数 open()。 开始分析 open_method() 的 kwargs 参数。发现其在相同的位置赋值为 &#123;&#125;。 查阅 open() 的文档，发现 encoding 参数默认为 None，具体值为 locale.getencoding()，查阅文档： locale.getencoding()获取当前的 locale encoding: 在 Android 和 VxWorks 上，将返回 &quot;utf-8&quot;。 在 Unix 上，将返回当前, return the encoding of the current LC_CTYPE 语言区域的编码格式。 如果 nl_langinfo(CODESET) 返回空字符串则将返回 &quot;utf-8&quot;: 举例来说，如果当前 LC_CTYPE 语言区域不受支持的时候。 在 Windows 上，返回 ANSI 代码页。 在 Windows 环境上测试，locale.getencoding() 返回 cp936，查阅资料获知即 gbk。 调用追踪gbk 的调用追踪： manage.py dumpdata django.core.management.commands.dumpdata open() locale.getencoding() 查看历史工单在申请工单之前，以关键词 dumpdata encoding 搜索历史工单，发现 #26721 与我所遇到的问题一致，已经关闭了。他们给出的解决方案是调整 Windows 的编码，如 如何在 Windows 上安装 Django 的“常见失误”一节所述。 总结本以为能搞个 Contribution 的，哪晓得这是前人早就研究过的东西。"},{"title":"TODO","date":"2023-10-31T07:30:00.000Z","url":"/2023/10/31/todos/","categories":[[" ",""]],"content":"记录自己品鉴过的精神食粮。 BooksReading Software Engineering after the Vibe Shift DevOps实践指南 领域驱动设计模式、原理与实践 To Read 项目管理之美 研究之美 代码之美 深入理解Kafka：核心设计与实践原理 演进式架构 Just for fun 持续交付 微服务架构设计模式 企业IT架构转型之道：阿里巴巴中台战略思想与架构实战 代码大全 硅谷增长黑客实战笔记 硅谷之迷 推荐系统 SRE：Google运维解密 发布！设计与部署稳定的分布式系统 构建高性能Web站点 经济学原理 宏观经济学分册 昆曼 经济学原理 微观经济学分册 昆曼 半小时漫画经济学 1~3 Software Engineering at Google 怎样解题 高效程序员的45个习惯：敏捷开发修炼之道 单元测试之道：使用JUnit 数据库系统概念 第六版 点石成金：访客至上的网页设计秘笈 计算机程序的构造和解释 第2版 高效程序员的 45 个习惯：敏捷开发修炼之道 A Computable Universe DOOM 启示录 The Ghost in the Quantum Turing Machine 开放的复杂巨系统 白帽子讲 Web 安全 纪念版 欺骗的艺术 码书：编码与解码的战争 黑客与画家 Getting Started With Dwarf Fortress Read 人件 华尔街漫步 ProGit 架构整洁之道 信息论、推理与学习算法 见识 小米创业思考 PostgreSQL修炼之道 剑指Offer：名企面试官精讲典型编程题 Redis 设计与实现 浪潮之巅 Effective Python：编写高质量Python代码的90个有效方法 第2版 投资学 大型网站技术架构：核心原理与案例分析 人月神话 40周年中文纪念版 数据密集型应用系统设计 SQL必知必会 流畅的 Python 第二版 Hacker’s Delight 2nd Edition Head First Java 中文第二版 Linux C 编程一站式学习 LinuxProbe 代码本色：用编程模拟自然系统 数学之美 第三版 复杂 复杂：诞生于秩序与混沌边缘的科学 大教堂与集市 深入理解计算机系统 第二版 算法导论 第三版 编码：隐匿在计算机软硬件背后的语言 算法竞赛入门经典 第二版 资深架构师学习笔记：学好C语言的扎实基本功 高质量 C++ C 编程指南 GalgamesPlayingTo Play 白色相簿2 天津罪 纸上的魔法使 纯白交响曲 近月少女的礼仪 秋之回忆：二重奏 幸福噩梦 遥仰凰华 Played 魔女的夜宴 千恋万花 梦末 Clannad 素晴日 樱之诗 ATRI 想要传达给你的爱恋 星空列车与白的旅行 水仙 候鸟 沙耶之歌 青空下的加缪 高考恋爱一百天 小白兔电商 雫 Clover Day’s Plus Doki Doki Literature Club Summer Pockets REFLECTION BLUE 苍之彼端的四重奏 Ever17 向日葵教会 Rewrite+ 秽翼的尤斯蒂娅 交响乐之雨 樱花萌放 海市蜃楼之馆 拔作岛 AnimeWatching 夏日口袋 拔作岛 To Watch 比宇宙更远的地方 Watched 魔卡少女樱 某科学的超电磁炮 某科学的超电磁炮S 狐妖小红娘 小林家的龙女仆 我是江小白 我是江小白 第二季 领风者 玉子爱情故事 非人哉 少女终末旅行 冰菓 CLANNAD CLANNAD 〜AFTER STORY〜 命运石之门 命运石之门 0 日常 日常 ETV版 日常 OAD 你的名字。 秒速5厘米 言叶之庭 魔法少女小圆 魔法少女小圆 剧场版 [新篇] 叛逆的物语 我的三体 我的三体 罗辑传 我的三体 章北海传 轻音少女 轻音少女 第二季 轻音少女 剧场版 少女☆歌剧 Revue Starlight 剧场版 少女☆歌剧 Revue Starlight 孤独摇滚！ BanG Dream! BanG Dream! 2nd Season BanG Dream! 少女乐团派对！☆PICO BanG Dream! 少女乐团派对！☆PICO～大份～ BanG Dream! 少女乐团派对！☆PICO Fever! BanG Dream! It’s MyGO!!!!! BanG Dream! Ave Mujica GIRLS BAND CRY 凉宫春日的忧郁 2009 凉宫春日的消失 幸运星 幸运星 OVA 东方梦想夏乡 幻想万华镜 红雾异变之章 幻想万华镜 春雪异变之章 幻想万华镜 永夜异变之章 末日时在做什么？有没有空？可以来拯救吗？ 我们仍未知道那天所看见的花的名字。 请问您今天要来点兔子吗？ 请问您今天要来点兔子吗？？ 败北女角太多了！ 义妹生活 葬送的芙莉莲 "},{"title":"About Me","date":"2024-09-08T05:48:43.000Z","url":"/about/index.html","categories":[[" ",""]],"content":"I’m😊: Backend Programmer I love😍: Open Source &amp; Community Work-Life Balance Challenging Tasks I’m good at😎: Chinese Helix Git Docker Python Django FastAPI I also can do some😉: English Kubernetes MySQL Postgres Redis Nginx Spark C&#x2F;C++ Java Rust Go RabbitMQ I’m learning🧐: English Kubernetes Postgres I also want to learn🤗: ElasticSearch Solr Hadoop Zookeeper etcd Kafka OAuth2 TiDB I will feel uncomfortable if I have to use😨: Go Java C++ C# Bash Windows Frontend Hardware Configure development environment I’m planning to do😋: Read more books. Build a high performance &amp; high avaliable system with DevOps. Do more practices. Learn economics. I can be found at😘: E-mail: &#104;&#97;&#105;&#114;&#108;&#x65;&#115;&#x73;&#118;&#x69;&#108;&#x6c;&#97;&#x67;&#101;&#x72;&#x40;&#x66;&#x6f;&#x78;&#x6d;&#x61;&#x69;&#x6c;&#x2e;&#99;&#111;&#109; GitHub:  Zhihu:  Bilibili:  "},{"title":"Friends","date":"2025-09-11T09:27:15.585Z","url":"/friends/index.html","categories":[[" ",""]]},{"date":"2025-09-11T09:27:15.614Z","url":"/search/index.html","categories":[[" ",""]]}]